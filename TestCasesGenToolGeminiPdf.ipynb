{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76b74a70",
   "metadata": {},
   "source": [
    "## genTestCasesTool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728e9afc",
   "metadata": {},
   "source": [
    "### Install package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13aaa84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U langchain-google-genai\n",
    "%pip install -U google-genai\n",
    "%pip install --upgrade --quiet  langchain-google-genai\n",
    "%pip install dotenv\n",
    "%pip install langchain-ollama\n",
    "%pip install --upgrade --quiet  langchain-ollama\n",
    "\n",
    "!pip install langchain-community\n",
    "!pip install unstructured\n",
    "!pip install pdfminer\n",
    "!pip install \"unstructured[all-docs]\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306872ca",
   "metadata": {},
   "source": [
    "## Connect Ollama Local model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5914b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI \n",
    "#from langchain_ollama import ChatOllama\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load= load_dotenv('../.env')\n",
    " \n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    # other params...\n",
    ")\n",
    "\n",
    "\n",
    "tc_llm = ChatGoogleGenerativeAI( \n",
    "   model=\"gemini-2.0-flash\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97c743b",
   "metadata": {},
   "source": [
    "## Create tools\n",
    "### Tools for Parallel Execution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4855293e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "import os\n",
    "from langchain_community.document_loaders import (\n",
    "    UnstructuredFileLoader\n",
    ")\n",
    "\n",
    "\n",
    "@tool\n",
    "def generate_test_cases_from_pdf(pdf_path: str) -> str:\n",
    "    \"Reads the requirement from the PDF and generates atleast 10 test cases scenarios in  BDD style format\"\n",
    "\n",
    "\n",
    "\n",
    "    Docs = \"Pdf\"\n",
    "    documents= []\n",
    "\n",
    "    for file in os.listdir(Docs):\n",
    "        filpath = os.path.join(Docs, file)\n",
    "        print(filpath)\n",
    "        loader = UnstructuredFileLoader(filpath)\n",
    "        #documents.extend(loader.load())\n",
    "        docs= loader.load()\n",
    "        for doc in docs:\n",
    "            #documents.append(f\"{filename} :: {doc.page_content.strip}\")\n",
    "            documents.append(f\"{file} :: {doc.page_content.strip()}\")\n",
    "\n",
    "    requirement_txt = \"\\n\\n\" .join(documents[:3])[:1000]\n",
    "  \n",
    "\n",
    "    prompt_template = PromptTemplate.from_template(\n",
    "        \"\"\"\n",
    "        You are a QA Automation Engineer. \n",
    "        Your task is to convert the following user story into at least 10 test cases.in Gherkin BDD style  format.\n",
    "        Include combinations of valid, invalid, and edge cases & alternative scenarios.\n",
    "\n",
    "        User Story: {requirement_txt}    \n",
    "\n",
    "        Format:\n",
    "        Feature: \n",
    "        Scenario:\n",
    "            Given \n",
    "            When \n",
    "            Then\n",
    "        And \n",
    "\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "                          \n",
    "\n",
    "    )\n",
    "    # âœ… Correct way to pass input\n",
    "    prompt = prompt_template.invoke({\"requirement_txt\": requirement_txt})\n",
    "    return llm.invoke(prompt) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1871d78a",
   "metadata": {},
   "source": [
    "## initialize_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f3ada30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_121001/60982014.py:3: LangChainDeprecationWarning: LangChain agents will continue to be supported, but it is recommended for new use cases to be built with LangGraph. LangGraph offers a more flexible and full-featured framework for building agents, including support for tool-calling, persistence of state, and human-in-the-loop workflows. For details, refer to the `LangGraph documentation <https://langchain-ai.github.io/langgraph/>`_ as well as guides for `Migrating from AgentExecutor <https://python.langchain.com/docs/how_to/migrate_agent/>`_ and LangGraph's `Pre-built ReAct agent <https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/>`_.\n",
      "  agent = initialize_agent(\n",
      "/tmp/ipykernel_121001/60982014.py:10: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = agent.run(\"requirements.pdf\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `generate_test_cases_from_pdf` with `{'pdf_path': 'requirements.pdf'}`\n",
      "\n",
      "\n",
      "\u001b[0mPdf/LLM_introduction.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_121001/1731837613.py:23: LangChainDeprecationWarning: The class `UnstructuredFileLoader` was deprecated in LangChain 0.2.8 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-unstructured package and should be used instead. To use it run `pip install -U :class:`~langchain-unstructured` and import as `from :class:`~langchain_unstructured import UnstructuredLoader``.\n",
      "  loader = UnstructuredFileLoader(filpath)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No languages specified, defaulting to English.\n",
      "\u001b[36;1m\u001b[1;3mcontent='As a QA Automation Engineer, I\\'ve interpreted the provided text, which describes a research paper, as a \"user story\" for an educational platform or documentation system that hosts and delivers this \"Self-contained Introduction to Large Language Models.\" The goal is to ensure this document effectively serves its intended audience (beginners with basic ML knowledge) and meets its described characteristics (self-contained, modularized).\\n\\nHere are 12 test cases in Gherkin BDD style:\\n\\n---\\n\\n**Feature: Learning about Large Language Models (LLMs) through the Self-contained Introduction**\\n\\n**Scenario 1: Successful Comprehension by Target Audience (Valid Case)**\\n    Given a user with basic machine learning knowledge\\n    When they access and read the \"Self-contained Introduction to Large Language Models\"\\n    Then they should be able to understand the fundamental concepts of LLMs.\\n\\n**Scenario 2: Navigating the Modularized Design (Valid Case)**\\n    Given a user is viewing the \"Self-contained Introduction to Large Language Models\"\\n    When they navigate through the document\\'s sections\\n    Then the content should follow a clear modularized design, progressing from a high-level overview to specific details.\\n\\n**Scenario 3: Self-contained Explanations of Basic Concepts (Valid Case)**\\n    Given a user is reading the \"Self-contained Introduction to Large Language Models\"\\n    When they encounter a basic machine learning concept mentioned in the text\\n    Then the document should provide a brief, self-contained explanation for that concept.\\n\\n**Scenario 4: Accessing Specific Model Details (GPT-2) (Valid Case)**\\n    Given a user is interested in detailed information about specific LLM architectures\\n    When they navigate to the relevant sections of the \"Self-contained Introduction to Large Language Models\"\\n    Then they should find comprehensive details regarding the GPT-2 model.\\n\\n**Scenario 5: Document Not Found (Invalid Case)**\\n    Given a user attempts to access the \"Self-contained Introduction to Large Language Models\"\\n    When the underlying document file (LLM_introduction.pdf) is unavailable or corrupted on the server\\n    Then the system should display an appropriate error message, such as \"Document Not Found\" or \"File Unavailable.\"\\n\\n**Scenario 6: User Lacks Prerequisite Knowledge (Invalid/Edge Case)**\\n    Given a user with no prior knowledge of machine learning\\n    When they attempt to read the \"Self-contained Introduction to Large Language Models\"\\n    Then they may find the content challenging to fully grasp, despite the document\\'s aim for beginners.\\n    And the platform might offer suggestions for prerequisite learning materials.\\n\\n**Scenario 7: Minimal Basic ML Knowledge (Edge Case)**\\n    Given a user with only minimal basic machine learning knowledge\\n    When they read the \"Self-contained Introduction to Large Language Models\"\\n    Then they should still be able to understand the core concepts of LLMs, relying on the document\\'s self-contained explanations.\\n\\n**Scenario 8: Advanced User Review (Edge Case)**\\n    Given an experienced AI researcher or practitioner\\n    When they quickly review the \"Self-contained Introduction to Large Language Models\"\\n    Then they should find it a concise and accurate high-level overview of LLMs.\\n    And they may find the detailed sections too basic for their expertise.\\n\\n**Scenario 9: Searching for the Introduction (Alternative Scenario)**\\n    Given a user is on the educational platform\\'s search interface\\n    When they search for terms like \"LLM introduction,\" \"Large Language Models,\" or \"Alice Lin LLM\"\\n    Then the \"Self-contained Introduction to Large Language Models\" document should appear as a prominent and relevant search result.\\n\\n**Scenario 10: Verifying Document Currency (Alternative Scenario)**\\n    Given a user accesses the \"Self-contained Introduction to Large Language Models\"\\n    When they check for publication or version information\\n    Then they should clearly see the date \"July 22, 2025\" or an equivalent indicator of its currency.\\n\\n**Scenario 11: Incomplete Modular Content (Invalid Case)**\\n    Given a user is navigating through the modular sections of the \"Self-contained Introduction to Large Language Models\"\\n    When a promised section, such as \"details of the GPT-2 model,\" is missing or leads to an empty page\\n    Then the user\\'s learning experience should be disrupted, and they cannot access the expected information.\\n\\n**Scenario 12: Providing Feedback on Content (Alternative Scenario)**\\n    Given a user has completed reading the \"Self-contained Introduction to Large Language Models\"\\n    When they wish to provide feedback on its clarity, accuracy, or completeness\\n    Then the platform should offer an accessible mechanism (e.g., a feedback form, comment section, or rating system) for them to submit their input.' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []} id='run--b93a4400-5f6e-4c5f-8806-fce9319bb969-0' usage_metadata={'input_tokens': 312, 'output_tokens': 4131, 'total_tokens': 4443, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 3122}}\u001b[0m\u001b[32;1m\u001b[1;3mHere are the 12 test cases in Gherkin BDD style generated from your `requirements.pdf` file:\n",
      "\n",
      "---\n",
      "\n",
      "**Feature: Learning about Large Language Models (LLMs) through the Self-contained Introduction**\n",
      "\n",
      "**Scenario 1: Successful Comprehension by Target Audience (Valid Case)**\n",
      "    Given a user with basic machine learning knowledge\n",
      "    When they access and read the \"Self-contained Introduction to Large Language Models\"\n",
      "    Then they should be able to understand the fundamental concepts of LLMs.\n",
      "\n",
      "**Scenario 2: Navigating the Modularized Design (Valid Case)**\n",
      "    Given a user is viewing the \"Self-contained Introduction to Large Language Models\"\n",
      "    When they navigate through the document's sections\n",
      "    Then the content should follow a clear modularized design, progressing from a high-level overview to specific details.\n",
      "\n",
      "**Scenario 3: Self-contained Explanations of Basic Concepts (Valid Case)**\n",
      "    Given a user is reading the \"Self-contained Introduction to Large Language Models\"\n",
      "    When they encounter a basic machine learning concept mentioned in the text\n",
      "    Then the document should provide a brief, self-contained explanation for that concept.\n",
      "\n",
      "**Scenario 4: Accessing Specific Model Details (GPT-2) (Valid Case)**\n",
      "    Given a user is interested in detailed information about specific LLM architectures\n",
      "    When they navigate to the relevant sections of the \"Self-contained Introduction to Large Language Models\"\n",
      "    Then they should find comprehensive details regarding the GPT-2 model.\n",
      "\n",
      "**Scenario 5: Document Not Found (Invalid Case)**\n",
      "    Given a user attempts to access the \"Self-contained Introduction to Large Language Models\"\n",
      "    When the underlying document file (LLM_introduction.pdf) is unavailable or corrupted on the server\n",
      "    Then the system should display an appropriate error message, such as \"Document Not Found\" or \"File Unavailable.\"\n",
      "\n",
      "**Scenario 6: User Lacks Prerequisite Knowledge (Invalid/Edge Case)**\n",
      "    Given a user with no prior knowledge of machine learning\n",
      "    When they attempt to read the \"Self-contained Introduction to Large Language Models\"\n",
      "    Then they may find the content challenging to fully grasp, despite the document's aim for beginners.\n",
      "    And the platform might offer suggestions for prerequisite learning materials.\n",
      "\n",
      "**Scenario 7: Minimal Basic ML Knowledge (Edge Case)**\n",
      "    Given a user with only minimal basic machine learning knowledge\n",
      "    When they read the \"Self-contained Introduction to Large Language Models\"\n",
      "    Then they should still be able to understand the core concepts of LLMs, relying on the document's self-contained explanations.\n",
      "\n",
      "**Scenario 8: Advanced User Review (Edge Case)**\n",
      "    Given an experienced AI researcher or practitioner\n",
      "    When they quickly review the \"Self-contained Introduction to Large Language Models\"\n",
      "    Then they should find it a concise and accurate high-level overview of LLMs.\n",
      "    And they may find the detailed sections too basic for their expertise.\n",
      "\n",
      "**Scenario 9: Searching for the Introduction (Alternative Scenario)**\n",
      "    Given a user is on the educational platform's search interface\n",
      "    When they search for terms like \"LLM introduction,\" \"Large Language Models,\" or \"Alice Lin LLM\"\n",
      "    Then the \"Self-contained Introduction to Large Language Models\" document should appear as a prominent and relevant search result.\n",
      "\n",
      "**Scenario 10: Verifying Document Currency (Alternative Scenario)**\n",
      "    Given a user accesses the \"Self-contained Introduction to Large Language Models\"\n",
      "    When they check for publication or version information\n",
      "    Then they should clearly see the date \"July 22, 2025\" or an equivalent indicator of its currency.\n",
      "\n",
      "**Scenario 11: Incomplete Modular Content (Invalid Case)**\n",
      "    Given a user is navigating through the modular sections of the \"Self-contained Introduction to Large Language Models\"\n",
      "    When a promised section, such as \"details of the GPT-2 model,\" is missing or leads to an empty page\n",
      "    Then the user's learning experience should be disrupted, and they cannot access the expected information.\n",
      "\n",
      "**Scenario 12: Providing Feedback on Content (Alternative Scenario)**\n",
      "    Given a user has completed reading the \"Self-contained Introduction to Large Language Models\"\n",
      "    When they wish to provide feedback on its clarity, accuracy, or completeness\n",
      "    Then the platform should offer an accessible mechanism (e.g., a feedback form, comment section, or rating system) for them to submit their input.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Here are the 12 test cases in Gherkin BDD style generated from your `requirements.pdf` file:\n",
      "\n",
      "---\n",
      "\n",
      "**Feature: Learning about Large Language Models (LLMs) through the Self-contained Introduction**\n",
      "\n",
      "**Scenario 1: Successful Comprehension by Target Audience (Valid Case)**\n",
      "    Given a user with basic machine learning knowledge\n",
      "    When they access and read the \"Self-contained Introduction to Large Language Models\"\n",
      "    Then they should be able to understand the fundamental concepts of LLMs.\n",
      "\n",
      "**Scenario 2: Navigating the Modularized Design (Valid Case)**\n",
      "    Given a user is viewing the \"Self-contained Introduction to Large Language Models\"\n",
      "    When they navigate through the document's sections\n",
      "    Then the content should follow a clear modularized design, progressing from a high-level overview to specific details.\n",
      "\n",
      "**Scenario 3: Self-contained Explanations of Basic Concepts (Valid Case)**\n",
      "    Given a user is reading the \"Self-contained Introduction to Large Language Models\"\n",
      "    When they encounter a basic machine learning concept mentioned in the text\n",
      "    Then the document should provide a brief, self-contained explanation for that concept.\n",
      "\n",
      "**Scenario 4: Accessing Specific Model Details (GPT-2) (Valid Case)**\n",
      "    Given a user is interested in detailed information about specific LLM architectures\n",
      "    When they navigate to the relevant sections of the \"Self-contained Introduction to Large Language Models\"\n",
      "    Then they should find comprehensive details regarding the GPT-2 model.\n",
      "\n",
      "**Scenario 5: Document Not Found (Invalid Case)**\n",
      "    Given a user attempts to access the \"Self-contained Introduction to Large Language Models\"\n",
      "    When the underlying document file (LLM_introduction.pdf) is unavailable or corrupted on the server\n",
      "    Then the system should display an appropriate error message, such as \"Document Not Found\" or \"File Unavailable.\"\n",
      "\n",
      "**Scenario 6: User Lacks Prerequisite Knowledge (Invalid/Edge Case)**\n",
      "    Given a user with no prior knowledge of machine learning\n",
      "    When they attempt to read the \"Self-contained Introduction to Large Language Models\"\n",
      "    Then they may find the content challenging to fully grasp, despite the document's aim for beginners.\n",
      "    And the platform might offer suggestions for prerequisite learning materials.\n",
      "\n",
      "**Scenario 7: Minimal Basic ML Knowledge (Edge Case)**\n",
      "    Given a user with only minimal basic machine learning knowledge\n",
      "    When they read the \"Self-contained Introduction to Large Language Models\"\n",
      "    Then they should still be able to understand the core concepts of LLMs, relying on the document's self-contained explanations.\n",
      "\n",
      "**Scenario 8: Advanced User Review (Edge Case)**\n",
      "    Given an experienced AI researcher or practitioner\n",
      "    When they quickly review the \"Self-contained Introduction to Large Language Models\"\n",
      "    Then they should find it a concise and accurate high-level overview of LLMs.\n",
      "    And they may find the detailed sections too basic for their expertise.\n",
      "\n",
      "**Scenario 9: Searching for the Introduction (Alternative Scenario)**\n",
      "    Given a user is on the educational platform's search interface\n",
      "    When they search for terms like \"LLM introduction,\" \"Large Language Models,\" or \"Alice Lin LLM\"\n",
      "    Then the \"Self-contained Introduction to Large Language Models\" document should appear as a prominent and relevant search result.\n",
      "\n",
      "**Scenario 10: Verifying Document Currency (Alternative Scenario)**\n",
      "    Given a user accesses the \"Self-contained Introduction to Large Language Models\"\n",
      "    When they check for publication or version information\n",
      "    Then they should clearly see the date \"July 22, 2025\" or an equivalent indicator of its currency.\n",
      "\n",
      "**Scenario 11: Incomplete Modular Content (Invalid Case)**\n",
      "    Given a user is navigating through the modular sections of the \"Self-contained Introduction to Large Language Models\"\n",
      "    When a promised section, such as \"details of the GPT-2 model,\" is missing or leads to an empty page\n",
      "    Then the user's learning experience should be disrupted, and they cannot access the expected information.\n",
      "\n",
      "**Scenario 12: Providing Feedback on Content (Alternative Scenario)**\n",
      "    Given a user has completed reading the \"Self-contained Introduction to Large Language Models\"\n",
      "    When they wish to provide feedback on its clarity, accuracy, or completeness\n",
      "    Then the platform should offer an accessible mechanism (e.g., a feedback form, comment section, or rating system) for them to submit their input.\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import initialize_agent, AgentType\n",
    "\n",
    "agent = initialize_agent(\n",
    "    tools=[generate_test_cases_from_pdf],\n",
    "    llm=llm,\n",
    "    agent=AgentType.OPENAI_FUNCTIONS,    \n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "result = agent.run(\"requirements.pdf\")\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
